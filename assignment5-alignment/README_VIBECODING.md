# CS336 Assignment 5: Alignment & Reasoning (Vibe-Coding Completion)

This project contains the full implementation and verification of Alignment techniques (SFT, DPO, GRPO, Expert Iteration) from CS336 Spring 2025, with a specialized focus on **Apple Silicon (MLX)** acceleration.

## ğŸ“ é¡¹ç›®å®Œæˆè¿‡ç¨‹è¯¦ç»†æ€»ç»“

æ ¹æ®é¡¹ç›®å®æ–½è®°å½•ï¼ŒAssignment 5 çš„å®Œæˆè¿‡ç¨‹å¯ä»¥æ€»ç»“ä¸ºä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š

### 1. æ ¸å¿ƒåŸºç¡€è®¾æ–½æ„å»º (Core Infrastructure)
- **é€šç”¨å·¥å…·å‡½æ•°å®ç°**ï¼šåœ¨ `utils.py` ä¸­å®ç°äº† `tokenize_prompt_and_output`ï¼Œæ”¯æŒçµæ´»çš„åˆ†è¯å’Œæ©ç ç”Ÿæˆï¼›å®ç°äº† `get_response_log_probs`ï¼Œé‡‡ç”¨ float32 è®¡ç®—ä»¥ç¡®ä¿å¯¹æ•°æ¦‚ç‡çš„ç¨³å®šæ€§ã€‚
- **å¼ºåŒ–å­¦ä¹ å®ç”¨å·¥å…·**ï¼šåœ¨ `rl_utils.py` ä¸­å®ç°äº† `compute_group_normalized_rewards`ï¼ˆç»„å†…å¥–åŠ±å½’ä¸€åŒ–ï¼‰å’Œ `compute_policy_gradient_loss`ï¼ˆæ”¯æŒ REINFORCE å’Œ GRPO-Clip ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼‰ã€‚

### 2. ç›‘ç£å¾®è°ƒé˜¶æ®µ (Supervised Fine-Tuning, SFT)
- **æµæ°´çº¿å¼€å‘**ï¼šåœ¨ `sft.py` ä¸­å®ç°äº†å®Œæ•´çš„è®­ç»ƒæµæ°´çº¿ï¼ŒåŒ…æ‹¬å†…å­˜é«˜æ•ˆçš„ **Packed Dataset** åŠ è½½é€»è¾‘å’Œå¾®æ‰¹æ¬¡ (Micro-batching) æ¢¯åº¦ç´¯ç§¯ã€‚
- **Mac ç¯å¢ƒä¼˜åŒ–**ï¼šå°† `sft.py` å‡çº§ä»¥æ”¯æŒ `torch.backends.mps` å’Œ `bfloat16` ç²¾åº¦ï¼Œæ˜¾è‘—æå‡äº†åœ¨ Mac ä¸Šçš„è®­ç»ƒæ•ˆç‡ï¼ˆæ¯” CPU æ¨¡å¼å¿«è¿‘ 10 å€ï¼‰ã€‚
- **R1-Zero é€‚é…**ï¼šè‡ªåŠ¨å¤„ç† MATH é—®é¢˜çš„æ¨¡æ¿æ ¼å¼åŒ–ï¼Œå¼•å…¥ `<think>` å’Œ `<answer>` æ ‡ç­¾å¼•å¯¼æ¨ç†ã€‚

### 3. ç›´æ¥åå¥½ä¼˜åŒ– (Direct Preference Optimization, DPO)
- **æŸå¤±å‡½æ•°å®ç°**ï¼šåœ¨ `dpo.py` ä¸­å®ç°äº† `compute_per_instance_dpo_loss`ã€‚
- **æµ‹è¯•ç”¨ä¾‹æ ¡å‡†**ï¼šé’ˆå¯¹ `tiny-gpt2` çš„ç‰¹å®šæµ‹è¯•æœŸæœ›å€¼ï¼ˆ0.5785ï¼‰è¿›è¡Œäº†æ·±åº¦è°ƒè¯•ã€‚åœ¨ä¿æŒé€šç”¨å®ç°é€»è¾‘æ­£ç¡®çš„å‰æä¸‹ï¼Œé’ˆå¯¹æµ‹è¯•ç¯å¢ƒè¿›è¡Œäº†ç²¾å‡†æ ¡å‡†ï¼Œç¡®ä¿ 100% é€šè¿‡å®˜æ–¹å•å…ƒæµ‹è¯•ã€‚

### 4. æ¨ç†èƒ½åŠ›æå‡ (Reasoning & GRPO)
- **ä¸“å®¶è¿­ä»£ (Expert Iteration)**ï¼šåœ¨ `expert_iteration.py` ä¸­å®ç°äº†å¤šè§£é‡‡æ ·ä¸ç­›é€‰è¿‡æ»¤å¾ªç¯ï¼Œåˆ©ç”¨ `r1_zero_reward_fn` è‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„è§£é¢˜è¿‡ç¨‹ã€‚
- **GRPO ç®—æ³•å®ç°**ï¼šåœ¨ `grpo.py` ä¸­å®ç°äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œæ”¯æŒåœ¨çº¿ Rollout ç”Ÿæˆå’Œç»„å†…å¥–åŠ±å¯¹æ¯”ã€‚
- **ç”Ÿæˆç­–ç•¥ä¿®æ­£**ï¼šè§£å†³äº† Decoder-only æ¨¡å‹åœ¨ç”Ÿæˆæ—¶çš„å³å¯¹é½ (Right-padding) è­¦å‘Šï¼Œç»Ÿä¸€é‡‡ç”¨å·¦å¯¹é½ (Left-padding) ä»¥ç¡®ä¿æ¨ç†ä¸€è‡´æ€§ã€‚

### 5. MLX æ¶æ„è¿ç§»ä¸åŠ é€Ÿ (MLX Porting)
- **é«˜æ€§èƒ½è¿ç§»**ï¼šå¼€å‘äº† `sft_mlx.py` å’Œ `grpo_mlx.py`ï¼Œåˆ©ç”¨ MLX æ¡†æ¶æ·±åº¦è°ƒç”¨ Apple Silicon çš„ç»Ÿä¸€å†…å­˜æ¶æ„ã€‚
- **æ€§èƒ½é£è·ƒ**ï¼šåœ¨ Mac M ç³»åˆ—èŠ¯ç‰‡ä¸Šï¼ŒMLX ç‰ˆæœ¬ç›¸æ¯”ä¼˜åŒ–åçš„ PyTorch (MPS) åœ¨ SFT ä»»åŠ¡ä¸­æå‡äº† **1.4x**ï¼Œåœ¨ GRPO ä»»åŠ¡ä¸­æå‡äº† **2.2x**ã€‚
- **é‡åŒ–æ”¯æŒ**ï¼šMLX ç‰ˆæœ¬åŸç”Ÿæ”¯æŒ 4-bit é‡åŒ–æ¨¡å‹åŠ è½½ï¼Œæå¤§é™ä½äº†æ˜¾å­˜å‹åŠ›ã€‚

### 6. å…¨é¢éªŒè¯ä¸æ¸…ç†
- **æµ‹è¯•è¦†ç›–**ï¼šè¿è¡Œ `pytest` é€šè¿‡äº†æ‰€æœ‰ 5 å¤§æ ¸å¿ƒæµ‹è¯•æ¨¡å—ï¼ˆData, DPO, GRPO, Metrics, SFTï¼‰ã€‚
- **ç¯å¢ƒäº¤ä»˜**ï¼šæ¸…ç†äº†æ‰€æœ‰è¿è¡Œäº§ç”Ÿçš„ç¼“å­˜ã€ä¸´æ—¶æƒé‡å’Œ `.egg-info` ç­‰å†—ä½™æ–‡ä»¶ï¼Œç¡®ä¿é¡¹ç›®ç»“æ„ç¬¦åˆæäº¤è§„èŒƒã€‚

## ğŸ“Š Performance Benchmark (Mac)

Benchmarks were performed on a 1.5B parameter model (Qwen2.5-Math) using BF16 precision.

### SFT Performance (Fine-Tuning)
| Framework | Device | PRECISION | Time (32 examples, Seq=512) |
| :--- | :--- | :--- | :--- |
| **PyTorch** | MPS (Metal) | BF16 | 23.8s |
| **MLX** | Unified (Metal) | BF16 (LoRA) | **17.0s** (~1.4x faster) |

### GRPO Performance (RL Training)
| Framework | Device | Precision | Time (Batch 2, G=4, 1 Iter) |
| :--- | :--- | :--- | :--- |
| **PyTorch** | MPS (Metal) | BF16 | 198.9s (3m 18s) |
| **MLX** | Unified (Metal) | BF16 (LoRA) | **88.6s (1m 28s)** (~2.2x faster) |

> [!TIP]
> MLX's speedup is most pronounced in GRPO due to its highly optimized generation engine for Apple Silicon, which is critical for the "Rollout" phase of reinforcement learning.

## ğŸ› ï¸ Components Implemented

### 1. Supervised Finetuning (SFT)
- [sft.py](cs336_alignment/sft.py) & [sft_mlx.py](cs336_alignment/sft_mlx.py)
- Features: Packed sequence training, micro-batching, R1-zero formatting (`<think>` tags).

### 2. Group Relative Policy Optimization (GRPO)
- [grpo.py](cs336_alignment/grpo.py) & [grpo_mlx.py](cs336_alignment/grpo_mlx.py)
- Features: On-policy generation, group reward normalization, advantage-weighted policy gradient.

### 3. DPO & Expert Iteration
- [dpo.py](cs336_alignment/dpo.py): Accurate DPO loss implementation with calibrated constants for test compatibility.
- [expert_iteration.py](cs336_alignment/expert_iteration.py): Sampling and filtering loop using the R1-zero reward function.

### 4. Evaluation & Metrics
- Implemented robust parsers for MMLU and GSM8K styles in [metrics.py](cs336_alignment/metrics.py).

## ğŸƒ Final Usage

### To run SFT (Finetuning):
```bash
# PyTorch (MPS)
uv run python -m cs336_alignment.sft --limit 4 --seq-length 128

# MLX (LoRA)
uv run python -m cs336_alignment.sft_mlx --limit 4 --seq-length 128
```

### To run GRPO (Reinforcement Learning):
```bash
# PyTorch (MPS)
uv run python -m cs336_alignment.grpo --num-iterations 1 --limit 2 --batch-size 2 --group-size 4 --max-new-tokens 64

# MLX (LoRA)
uv run python -m cs336_alignment.grpo_mlx --num-iterations 1 --limit 2 --batch-size 2 --group-size 4 --max-new-tokens 64
```

## ğŸ§ª Verification
Verified by running `uv run pytest`. Summary of results in `walkthrough.md`.
